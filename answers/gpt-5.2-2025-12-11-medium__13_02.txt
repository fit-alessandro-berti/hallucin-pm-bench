1) **Occurrences of “Manual_Fraud_Check”**: **1** (in case **C05**).

2) **Percentages including that activity** (rounded to two decimals)  
   a) Of all **events** (48 total): \( \frac{1}{48} \times 100 = 2.08\% \) → **2.08%**  
   b) Of all **unique cases** (12 total): \( \frac{1}{12} \times 100 = 8.33\% \) → **8.33%**

3) **Why low support can mislead (≤100 words)**  
With only 1 occurrence (1 case), “Manual_Fraud_Check” is statistically unstable: its observed timing and outcome links (e.g., fast approval in C05) may be idiosyncratic, not representative. If we generalise, performance metrics (average durations, bottlenecks) can be skewed by a single atypical path, and predictive models may overfit—treating a rare step as a strong signal for approval/rejection—leading to poor generalisation on the full process population.