1) **Occurrences of “Manual_Fraud_Check”**: **1** (in case **C05**)

2) **Percentages including that activity** (rounded to 2 decimals)  
- a) Of **all events (48)**: \( \frac{1}{48}\times 100 = \) **2.08%**  
- b) Of **all cases (12)**: \( \frac{1}{12}\times 100 = \) **8.33%**

3) **Why low support can mislead (≤100 words)**  
Manual_Fraud_Check appears only once (1/48 events; 1/12 cases). Any performance metric for that step (duration, waiting time, rework) is therefore based on a single trace and can be an outlier, skewing averages or bottleneck rankings. In predictive models, the activity becomes a very sparse feature; the model may overfit to that one case or ignore it entirely, yielding unstable estimates and poor generalisation. From this small slice, its true frequency and impact cannot be inferred reliably.