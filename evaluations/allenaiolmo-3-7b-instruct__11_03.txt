**2.0**

### Evaluation Rationale (strict assessment vs. prompt & ground truth):
- **Core failure in activity selection (jointly SLA breaches + high waiting; -4.0)**: Prompt requires *three worst-performing* based on data. Ground truth correctly identifies Legal Approval (+300s breach, 600s wait), Risk Review (+300s, 480s), Credit Assessment (+60s, 200s)—top by breach magnitude *and* waiting. LLM wrongly picks Receive Application (45s throughput *<60s SLA*, **no breach**, 10s wait; falsely claims +60s breach). This inverts data, picking a top performer as "worst."
- **Quantification errors (-1.5)**: Credit/Legal partially correct (+60s/+300s), but Legal omits throughput (900s); Receive fully invented (+60s breach).
- **Recommendations (-1.5)**: Not "data-driven" (prompt: ground in data); generic ("parallel workflows," "automated alerts," "pre-verification queue") vs. ground truth's concrete (e.g., "rule-based pre-checks," "add one risk analyst"). Wrong activity for third.
- **Format/structure deviations (-0.5)**: Bullets only for recs (ok), but adds signature ("Oliver"), "Dear COO," casual phrasing; ≠ ground truth's To/From/Subject/bold headers. ~100 words vs. ~150.
- **Other violations (-0.5)**: Invents non-compliance for Receive; no total cases/projection like ground truth.
- **Partial credits**: Two activities/breaches correct (+1.0 total); memo-like, data-grounded for most.

Major factual errors on core task (wrong activities/breaches) warrant low score; small issues compound under "utmost strictness."