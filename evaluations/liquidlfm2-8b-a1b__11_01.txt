2.0

### Evaluation Breakdown
- **Correctness (activities, ranking, numbers)**: Severe failures. The LLM incorrectly ranks Fraud_Check as #1 with Assess_Liability's metrics (7.8h avg, 16.4h p95), instead of its actual low values (0.9h, 2.5h). Request_Documents is demoted to #2 (should be #1) with correct metrics, and Assess_Liability is correctly metrics'd but wrongly ranked #3 while missing the true #2 (Send_Closure_Letter: 12.2h, 44h). This inverts the top three, violating strict table matching. (-6 points)
- **No unwarranted content**: Compliant—no service times, throughput, or extra activities discussed. (+1 point)
- **Action relevance**: Actions are somewhat logical and claim ≥20% reduction, but lack data-driven substantiation (e.g., no pilots, estimates, or tests as in ground truth). Tied to wrong activities (e.g., AI for Fraud_Check ignores its low waits). (-1 point)
- **Overall adherence (format, length, focus)**: Memo format is crisp and ≤150 words (~120), focuses only on waits. But core prompt violations dominate. Total: Minimal partial credit for partial Request_Documents handling and structure.